{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import soundfile as sf\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from tinydb import TinyDB, Query\n",
    "\n",
    "sys.path.append('/export/home/jrichter/Projects/audio_visual/frameworks/uhh-sp')\n",
    "sys.path.append('/export/home/jrichter/Projects/audio_visual/python')\n",
    "\n",
    "from uhh_sp.dsp import stft, istft\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Temporal Convolutional Network (STCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2, \n",
    "                 activation=nn.ReLU()):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.resample = (weight_norm(nn.Conv1d(in_channels, out_channels, 1)) \n",
    "                         if in_channels != out_channels else None)\n",
    "        self.padding = nn.ConstantPad1d(((kernel_size - 1) * dilation, 0), 0)\n",
    "        self.convolution = weight_norm(nn.Conv1d(out_channels, out_channels, \n",
    "                                                 kernel_size, dilation=dilation))\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)     \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.convolution.weight.data.normal_(0, 0.01)\n",
    "        if self.resample is not None:\n",
    "            self.resample.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x if self.resample is None else self.resample(x)\n",
    "        y = self.dropout(self.activation(self.convolution(self.padding(x))))\n",
    "        return self.activation(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, dropout=0.2, activation=nn.ReLU()):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(*[ResidualBlock(channels[i], channels[i+1],\n",
    "            kernel_size, 2**i, dropout, activation) for i in range(len(channels) - 1)]) \n",
    "                                    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def representation(self, x, level):\n",
    "        for i in range(level):\n",
    "            x = self.layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentLayer(nn.Module):\n",
    "    def __init__(self, channels, kernel_size):\n",
    "        super(LatentLayer, self).__init__()\n",
    "        \n",
    "        self.latent_variables = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STCN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, dropout=0.2, activation=nn.ReLU()):\n",
    "        super(STCN, self).__init__()\n",
    "        \n",
    "        self.network = TCN(channels, kernel_size, dropout, activation)\n",
    "        self.latent_variables = LatentVariables(channels, kernel_size)\n",
    "    \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # encoder  \n",
    "        self.enc_in = nn.Linear(in_out_dim, hidden_dim)\n",
    "        self.enc_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers)])\n",
    "        self.enc_out_1 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.enc_out_2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.dec_in = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.dec_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers)])\n",
    "        self.dec_out = nn.Linear(hidden_dim, in_out_dim)\n",
    "              \n",
    "    def encode(self, x):\n",
    "        h = torch.tanh(self.enc_in(x))\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            h = torch.tanh(self.enc_hidden[i](h))     \n",
    "        return self.enc_out_1(h), self.enc_out_2(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.tanh(self.dec_in(z))\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            h = torch.tanh(self.dec_hidden[i](h))\n",
    "        return self.dec_out(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 513))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STCN(\n",
       "  (deterministic_network): TCN(\n",
       "    (layers): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (resample): Conv1d(2, 5, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(2, 0), value=0)\n",
       "        (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,))\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (padding): ConstantPad1d(padding=(4, 0), value=0)\n",
       "        (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (padding): ConstantPad1d(padding=(8, 0), value=0)\n",
       "        (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (resample): Conv1d(5, 3, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(16, 0), value=0)\n",
       "        (convolution): Conv1d(3, 3, kernel_size=(3,), stride=(1,), dilation=(8,))\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stochastic_variables): LatentVariables(\n",
       "    (latent_variables): TCN(\n",
       "      (layers): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (resample): Conv1d(2, 5, kernel_size=(1,), stride=(1,))\n",
       "          (padding): ConstantPad1d(padding=(2, 0), value=0)\n",
       "          (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (padding): ConstantPad1d(padding=(4, 0), value=0)\n",
       "          (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (2): ResidualBlock(\n",
       "          (padding): ConstantPad1d(padding=(8, 0), value=0)\n",
       "          (convolution): Conv1d(5, 5, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (3): ResidualBlock(\n",
       "          (resample): Conv1d(5, 3, kernel_size=(1,), stride=(1,))\n",
       "          (padding): ConstantPad1d(padding=(16, 0), value=0)\n",
       "          (convolution): Conv1d(3, 3, kernel_size=(3,), stride=(1,), dilation=(8,))\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = [2, 5, 5, 5, 3]\n",
    "kernel_size = 3\n",
    "dropout = 0.0\n",
    "activation = lambda x : x\n",
    "\n",
    "model = STCN(channels, kernel_size, dropout, activation)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9051, -0.4287, -0.5468, -0.5347, -0.6750, -1.2396, -0.6867,\n",
       "          -0.9820, -0.8129, -0.2056],\n",
       "         [-0.3419, -0.6165, -0.3817, -0.3315, -0.6376, -0.0914, -0.8208,\n",
       "          -0.0322, -0.0062, -0.0756],\n",
       "         [ 0.0613, -0.2904,  0.0367,  0.1918, -0.2204,  0.8778, -0.2903,\n",
       "           0.6046,  0.4820, -0.0654]],\n",
       "\n",
       "        [[-0.4264, -0.4999, -0.8912, -0.7094, -0.8899, -1.4694, -0.9052,\n",
       "          -0.5631, -0.8975, -0.6074],\n",
       "         [-0.3022, -0.5099, -0.3062, -0.1465, -0.2287, -0.1667, -0.2507,\n",
       "          -0.4289,  0.1311, -0.3389],\n",
       "         [-0.0573, -0.1170,  0.3888,  0.4377,  0.3673,  0.8339,  0.2395,\n",
       "          -0.1399,  0.4895, -0.2250]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_length = 10\n",
    "\n",
    "input = torch.randn(batch_size, in_channels, seq_length)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
