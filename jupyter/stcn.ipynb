{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Temporal Convolutional Network (STCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2, \n",
    "                 activation=nn.ReLU()):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.resample = (nn.utils.weight_norm(nn.Conv1d(in_channels, out_channels, 1)) \n",
    "                         if in_channels != out_channels else None)\n",
    "        self.padding = nn.ConstantPad1d(((kernel_size - 1) * dilation, 0), 0)\n",
    "        self.convolution = nn.utils.weight_norm(nn.Conv1d(out_channels, out_channels, \n",
    "                                                 kernel_size, dilation=dilation))\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)     \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.convolution.weight.data.normal_(0, 0.01)\n",
    "        if self.resample is not None:\n",
    "            self.resample.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x if self.resample is None else self.resample(x)\n",
    "        y = self.dropout(self.activation(self.convolution(self.padding(x))))\n",
    "        return self.activation(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, dropout=0.2, activation=nn.ReLU()):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        \n",
    "        self.layers = nn.Sequential(*[ResidualBlock(channels[i], channels[i+1],\n",
    "            kernel_size, 2**i, dropout, activation) for i in range(len(channels)-1)]) \n",
    "    \n",
    "    def representations(self, x):\n",
    "        # bottom-up\n",
    "        d = [x]\n",
    "        for i in range(len(self.channels)-1):\n",
    "            d += [self.layers[i](d[-1])]\n",
    "        return d[1:]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_hidden_layers):\n",
    "        super(ObservationModel, self).__init__()\n",
    "        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        self.dec_in = nn.Conv1d(input_dim, hidden_dim, 1)\n",
    "        self.dec_hidden = nn.Sequential(*[nn.Conv1d(hidden_dim, hidden_dim, 1) \n",
    "                                         for _ in range(num_hidden_layers)])\n",
    "        self.dec_out_1 = nn.Conv1d(hidden_dim, output_dim, 1)\n",
    "        self.dec_out_2 = nn.Conv1d(hidden_dim, output_dim, 1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.tanh(self.dec_in(z))\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            h = torch.tanh(self.dec_hidden[i](h))     \n",
    "        return self.dec_out_1(h), self.dec_out_2(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = torch.cat(z, dim=1)\n",
    "        mu, logvar = self.decode(z)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentLayer(nn.Module):\n",
    "    def __init__(self, tcn_dim, latent_dim_in, latent_dim_out, hidden_dim, num_hidden_layers):\n",
    "        super(LatentLayer, self).__init__()\n",
    "        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        self.enc_in = nn.Conv1d(tcn_dim+latent_dim_in, hidden_dim, 1)\n",
    "        self.enc_hidden = nn.Sequential(*[nn.Conv1d(hidden_dim, hidden_dim, 1) \n",
    "                                         for _ in range(num_hidden_layers)])\n",
    "        self.enc_out_1 = nn.Conv1d(hidden_dim, latent_dim_out, 1)\n",
    "        self.enc_out_2 = nn.Conv1d(hidden_dim, latent_dim_out, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.enc_in(x))\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            h = torch.tanh(self.enc_hidden[i](h))     \n",
    "        return self.enc_out_1(h), self.enc_out_2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel(nn.Module):\n",
    "    def __init__(self, tcn_channels, latent_channels, num_hidden_layers):\n",
    "        super(GenerativeModel, self).__init__()\n",
    "        \n",
    "        self.layers = [LatentLayer(tcn_channels[i], latent_channels[i+1], latent_channels[i], \n",
    "                              latent_channels[i], num_hidden_layers) for i in range(len(tcn_channels)-1)]  \n",
    "        self.layers += [LatentLayer(tcn_channels[-1], 0, latent_channels[-1], latent_channels[-1], \n",
    "                                num_hidden_layers)]\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "                \n",
    "    def forward(self, d):\n",
    "        # top-down\n",
    "        _mu, _logvar = self.layers[-1](d[-1])\n",
    "        mu = [_mu]; logvar = [_logvar]\n",
    "        z = [self.reparameterize(_mu, _logvar)]\n",
    "        for i in reversed(range(len(self.layers)-1)):\n",
    "            _mu, _logvar = self.layers[i](torch.cat((d[i], z[-1]), dim=1))\n",
    "            z += [self.reparameterize(_mu, _logvar)]\n",
    "            mu = [_mu] + mu\n",
    "            logvar = [_logvar] + logvar\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self, tcn_channels, latent_channels, num_hidden_layers):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        \n",
    "        self.layers = [LatentLayer(tcn_channels[i], latent_channels[i+1], latent_channels[i], \n",
    "                              latent_channels[i], num_hidden_layers) for i in range(len(tcn_channels)-1)]  \n",
    "        self.layers += [LatentLayer(tcn_channels[-1], 0, latent_channels[-1], latent_channels[-1], \n",
    "                                num_hidden_layers)]\n",
    "        self.layers = nn.ModuleList(self.layers)   \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    def forward(self, d, mu_p, logvar_p):\n",
    "        # top-down\n",
    "        mu_q_hat, logvar_q_hat = self.layers[-1](d[-1])   \n",
    "        logvar_q = torch.log(1/(torch.pow(torch.exp(logvar_q_hat), -2) \n",
    "                                + torch.pow(torch.exp(logvar_p[-1]), -2)))\n",
    "        mu_q = logvar_q*(mu_q_hat*torch.sqrt(logvar_q_hat) + mu_p[-1]*torch.sqrt(logvar_p[-1]))      \n",
    "        z = [self.reparameterize(mu_q, logvar_q)]\n",
    "        for i in reversed(range(len(self.layers)-1)):\n",
    "            mu_q_hat, logvar_q_hat = self.layers[i](torch.cat((d[i], z[-1]), dim=1))\n",
    "            logvar_q = torch.log(1/(torch.pow(torch.exp(logvar_q_hat), -2) \n",
    "                                + torch.pow(torch.exp(logvar_p[i]), -2)))\n",
    "            mu_q = logvar_q*(mu_q_hat * torch.sqrt(logvar_q_hat) + mu_p[i] * torch.sqrt(logvar_p[i]))             \n",
    "            z += [self.reparameterize(mu_q, logvar_q)]        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STCN(nn.Module):\n",
    "    def __init__(self, input_dim, tcn_channels, latent_channels, mode='inference',\n",
    "                 kernel_size=2, dropout=0.2, activation=nn.ReLU()):\n",
    "        super(STCN, self).__init__()\n",
    "        \n",
    "        self.mode = mode    \n",
    "        self.tcn = TCN([input_dim]+tcn_channels, kernel_size, dropout, activation)   \n",
    "        self.generative_model = GenerativeModel(tcn_channels, latent_channels, \n",
    "                                                num_hidden_layers=2)\n",
    "        self.inference_model = InferenceModel(tcn_channels, latent_channels, \n",
    "                                                num_hidden_layers=2)    \n",
    "        self.observation_model = ObservationModel(input_dim=sum(latent_channels), output_dim=input_dim, \n",
    "                                                  hidden_dim=256, num_hidden_layers=5)\n",
    "        \n",
    "    def generate(self, x):\n",
    "        d = self.tcn.representations(x) \n",
    "        d_shift = [(nn.functional.pad(d[i], pad=(1, 0))[:,:,:-1]) for i in range(len(d))]  \n",
    "        \n",
    "        z_p, _, _ = self.generative_model(d_shift)\n",
    "        x_hat = self.observation_model(z_p)          \n",
    "        return x_hat\n",
    "    \n",
    "    def infer(self, x):\n",
    "        d = self.tcn.representations(x) \n",
    "        d_shift = [(nn.functional.pad(d[i], pad=(1, 0))[:,:,:-1]) for i in range(len(d))]  \n",
    "        \n",
    "        z_p, mu_p, logvar_p = self.generative_model(d_shift)\n",
    "        z_q = self.inference_model(d, mu_p, logvar_p)\n",
    "        x_hat = self.observation_model(z_q)          \n",
    "        return x_hat\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mode == 'inference':\n",
    "            x_hat = self.infer(x)\n",
    "        elif self.mode == 'generation':\n",
    "            x_hat = self.generate(x)\n",
    "        else:\n",
    "            return None\n",
    "        return x_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_dim = 256\n",
    "tcn_channels = [128, 64, 32, 16]\n",
    "latent_channels = [64, 32, 16, 8]\n",
    "\n",
    "model = STCN(input_dim, tcn_channels, latent_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 1000\n",
    "\n",
    "input = torch.randn(batch_size, input_dim, seq_length)\n",
    "prediction = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STCN(\n",
       "  (tcn): TCN(\n",
       "    (layers): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (resample): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(1, 0), value=0)\n",
       "        (convolution): Conv1d(128, 128, kernel_size=(2,), stride=(1,))\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (resample): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(2, 0), value=0)\n",
       "        (convolution): Conv1d(64, 64, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (resample): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(4, 0), value=0)\n",
       "        (convolution): Conv1d(32, 32, kernel_size=(2,), stride=(1,), dilation=(4,))\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (resample): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "        (padding): ConstantPad1d(padding=(8, 0), value=0)\n",
       "        (convolution): Conv1d(16, 16, kernel_size=(2,), stride=(1,), dilation=(8,))\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (generative_model): GenerativeModel(\n",
       "    (layers): ModuleList(\n",
       "      (0): LatentLayer(\n",
       "        (enc_in): Conv1d(160, 64, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): LatentLayer(\n",
       "        (enc_in): Conv1d(80, 32, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): LatentLayer(\n",
       "        (enc_in): Conv1d(40, 16, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): LatentLayer(\n",
       "        (enc_in): Conv1d(16, 8, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inference_model): InferenceModel(\n",
       "    (layers): ModuleList(\n",
       "      (0): LatentLayer(\n",
       "        (enc_in): Conv1d(160, 64, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): LatentLayer(\n",
       "        (enc_in): Conv1d(80, 32, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): LatentLayer(\n",
       "        (enc_in): Conv1d(40, 16, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): LatentLayer(\n",
       "        (enc_in): Conv1d(16, 8, kernel_size=(1,), stride=(1,))\n",
       "        (enc_hidden): Sequential(\n",
       "          (0): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (enc_out_1): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "        (enc_out_2): Conv1d(8, 8, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (observation_model): ObservationModel(\n",
       "    (dec_in): Conv1d(120, 256, kernel_size=(1,), stride=(1,))\n",
       "    (dec_hidden): Sequential(\n",
       "      (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (dec_out_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (dec_out_2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
